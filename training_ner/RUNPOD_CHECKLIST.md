# Checklist de d√©ploiement RunPod

## ‚úÖ Pr√©-d√©ploiement (Local)

### 1. Validation setup
- [ ] `python validate_setup.py` passe tous les tests
- [ ] Config YAML valide et compl√®te
- [ ] Donn√©es JSONL bien format√©es
- [ ] Labels coh√©rents (label2id.json)
- [ ] Dependencies list√©es dans requirements.txt

### 2. V√©rification donn√©es
- [ ] train.jsonl : minimum 1000 exemples
- [ ] val.jsonl : minimum 100 exemples  
- [ ] test.jsonl : minimum 100 exemples
- [ ] Tous les labels pr√©sents dans label2id.json
- [ ] Pas de tokens vides ou manquants

### 3. Backup
- [ ] Commit git de tout le code
- [ ] Sauvegarder donn√©es localement
- [ ] Noter la config exacte utilis√©e

## üöÄ D√©ploiement RunPod

### 1. Cr√©ation instance
- [ ] GPU s√©lectionn√© : RTX 4090 / A100
- [ ] Template : PyTorch 2.0+ CUDA 11.8+
- [ ] Volume persistant : 50 GB minimum
- [ ] Ports : SSH (22), Jupyter (8888)

### 2. Upload code et donn√©es
```bash
# Option 1: Git clone
git clone <your_repo> /workspace/training_ner

# Option 2: SCP
scp -r training_ner/ root@<runpod_ip>:/workspace/
```

- [ ] Code upload√© dans `/workspace/training_ner`
- [ ] Donn√©es dans `/workspace/training_ner/data/`
- [ ] Config dans `/workspace/training_ner/configs/`

### 3. Installation d√©pendances
```bash
cd /workspace/training_ner
pip install -r requirements.txt
```

- [ ] Toutes les d√©pendances install√©es
- [ ] CUDA disponible (`python -c "import torch; print(torch.cuda.is_available())"`)
- [ ] Transformers install√©

### 4. Test rapide
```bash
# Valider setup
python validate_setup.py

# Test teacher loading (rapide)
python -c "from models import load_teacher; from utils import load_config; teacher, tok = load_teacher(load_config('configs/kd_camembert.yaml'))"
```

- [ ] Validation passe
- [ ] Teacher se charge sans erreur

## üéØ Entra√Ænement

### 1. Lancer distillation
```bash
# Screen ou tmux pour √©viter d√©connexion
screen -S training
python train_kd.py --config configs/kd_camembert.yaml --output /workspace/artifacts/student_10L
```

- [ ] Training lanc√©
- [ ] Logs visibles
- [ ] GPU utilis√© (v√©rifier avec `nvidia-smi`)

### 2. Monitoring
```bash
# Dans un autre terminal
watch -n 30 nvidia-smi
tail -f /workspace/artifacts/student_10L/training_log.jsonl
```

- [ ] GPU memory stable (pas de OOM)
- [ ] Loss diminue
- [ ] Temps par epoch raisonnable (~30-60 min)

### 3. Checkpoints
- [ ] Checkpoints sauvegard√©s r√©guli√®rement
- [ ] Validation loss logged
- [ ] Pas d'erreurs dans les logs

## üî™ Pruning

### 1. Apr√®s distillation compl√®te
```bash
python prune_heads.py \
  --model /workspace/artifacts/student_10L \
  --rate 0.25 \
  --output /workspace/artifacts/student_10L_pruned
```

- [ ] Pruning termin√©
- [ ] Masque sauvegard√© (heads_pruned_mask.json)

### 2. Fine-tuning post-pruning
```bash
python finetune_postprune.py \
  --model /workspace/artifacts/student_10L_pruned \
  --output /workspace/artifacts/student_10L_final \
  --epochs 2
```

- [ ] Fine-tuning termin√©
- [ ] Mod√®le final sauvegard√©

## üîç Test et validation

### 1. Inf√©rence test
```bash
# Cr√©er fichier de test
echo "Jean Dupont habite √† Paris." > /workspace/test.txt
echo "Le Dr Martin travaille √† Lyon." >> /workspace/test.txt

# Inf√©rence
python inference.py \
  --model /workspace/artifacts/student_10L_final \
  --input /workspace/test.txt \
  --output /workspace/results.jsonl
  
# V√©rifier r√©sultats
cat /workspace/results.jsonl
```

- [ ] Inf√©rence fonctionne
- [ ] Entit√©s d√©tect√©es coh√©rentes
- [ ] Format JSONL correct

### 2. √âvaluation (optionnel)
```bash
# TODO: Script d'√©valuation √† cr√©er
python evaluate.py \
  --model /workspace/artifacts/student_10L_final \
  --test_file data/test.jsonl
```

- [ ] F1-score calcul√©
- [ ] Performances acceptables (>85%)

## üì• R√©cup√©ration mod√®le

### 1. Download depuis RunPod
```bash
# Depuis votre machine locale
scp -r root@<runpod_ip>:/workspace/artifacts/student_10L_final ./models/
```

- [ ] Mod√®le t√©l√©charg√©
- [ ] Tous les fichiers pr√©sents (pytorch_model.bin, config.json, tokenizer)

### 2. Test local
```python
from transformers import AutoTokenizer, AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained("./models/student_10L_final")
tokenizer = AutoTokenizer.from_pretrained("./models/student_10L_final")

# Test
text = "Jean habite √† Paris"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
print(outputs.logits.shape)
```

- [ ] Mod√®le charge localement
- [ ] Inf√©rence fonctionne

## üßπ Cleanup RunPod

### 1. Sauvegarder artefacts importants
- [ ] Mod√®le final t√©l√©charg√©
- [ ] Logs de training sauvegard√©s
- [ ] Masque de pruning sauvegard√©
- [ ] Config finale sauvegard√©e

### 2. Arr√™ter instance
- [ ] Stop pod (si volume persistant)
- [ ] Terminate pod (si plus besoin)

## üìä Documentation r√©sultats

### M√©triques √† noter
- [ ] Temps total d'entra√Ænement
- [ ] Loss finale (train/val)
- [ ] F1-score final (si √©valu√©)
- [ ] Taille mod√®le (param√®tres)
- [ ] Compression ratio vs teacher
- [ ] Vitesse inf√©rence (tokens/sec)

### Probl√®mes rencontr√©s
- [ ] Documenter erreurs rencontr√©es
- [ ] Solutions appliqu√©es
- [ ] Hyperparam√®tres ajust√©s

## üéØ Next steps

- [ ] D√©ployer mod√®le en production
- [ ] Cr√©er API REST pour inf√©rence
- [ ] Monitorer performances en production
- [ ] Collecter feedback utilisateurs
- [ ] It√©rer sur nouvelles donn√©es

---

**Date de d√©ploiement**: ___________
**Dur√©e totale**: ___________
**Co√ªt RunPod**: ___________
**R√©sultats**: ‚úÖ / ‚ö†Ô∏è / ‚ùå
