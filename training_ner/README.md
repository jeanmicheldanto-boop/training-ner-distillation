# Training NER - Knowledge Distillation Pipeline

Pipeline complet de distillation et pruning pour modÃ¨le NER franÃ§ais mÃ©dico-social.

## ğŸ¯ Objectif

CrÃ©er un modÃ¨le student lÃ©ger (10 couches, 25% tÃªtes prunÃ©es) Ã  partir du teacher `Jean-Baptiste/camembert-ner` pour dÃ©ploiement sur GPU RunPod.

## ğŸ“ Structure

```
training_ner/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ kd_camembert.yaml          # Configuration complÃ¨te
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.jsonl                # DonnÃ©es d'entraÃ®nement
â”‚   â”œâ”€â”€ val.jsonl                  # DonnÃ©es de validation
â”‚   â”œâ”€â”€ test.jsonl                 # DonnÃ©es de test
â”‚   â””â”€â”€ label2id.json              # Mapping labels NER
â”œâ”€â”€ artifacts/                      # ModÃ¨les sauvegardÃ©s
â”‚   â”œâ”€â”€ student_10L/               # Student aprÃ¨s distillation
â”‚   â”œâ”€â”€ student_10L_pruned/        # Student aprÃ¨s pruning
â”‚   â””â”€â”€ student_10L_final/         # Student aprÃ¨s fine-tuning
â”œâ”€â”€ models.py                       # DÃ©finition Teacher/Student
â”œâ”€â”€ losses.py                       # Pertes de distillation
â”œâ”€â”€ pruning.py                      # Pruning tÃªtes d'attention
â”œâ”€â”€ data_loader.py                  # Chargement donnÃ©es
â”œâ”€â”€ utils.py                        # Utilitaires
â”œâ”€â”€ train_kd.py                     # Script distillation
â”œâ”€â”€ prune_heads.py                  # Script pruning
â”œâ”€â”€ finetune_postprune.py          # Script fine-tuning
â””â”€â”€ inference.py                    # Script infÃ©rence
```

## ğŸ“‹ Format des donnÃ©es

### JSONL Format (train.jsonl, val.jsonl, test.jsonl)
```json
{"tokens": ["Jean", "habite", "Ã ", "Paris"], "ner_tags": ["B-PER", "O", "O", "B-LOC"]}
{"tokens": ["Le", "Dr", "Martin", "travaille"], "ner_tags": ["O", "O", "B-PER", "O"]}
```

### label2id.json
```json
{
  "O": 0,
  "B-PER": 1,
  "I-PER": 2,
  "B-LOC": 3,
  "I-LOC": 4,
  "B-ORG": 5,
  "I-ORG": 6
}
```

## ğŸš€ Pipeline d'utilisation

### 1. Distillation (Knowledge Distillation)

EntraÃ®ner le student avec le teacher comme rÃ©fÃ©rence.

```bash
python train_kd.py \
  --config configs/kd_camembert.yaml \
  --output artifacts/student_10L
```

**DurÃ©e estimÃ©e**: 4-8 heures (10 epochs sur GPU A100)

**Sorties**:
- `artifacts/student_10L/pytorch_model.bin`
- `artifacts/student_10L/config.json`
- `artifacts/student_10L/tokenizer_config.json`
- `artifacts/student_10L/training_log.jsonl`

### 2. Pruning (25% tÃªtes d'attention)

Pruner les tÃªtes les moins importantes.

```bash
python prune_heads.py \
  --model artifacts/student_10L \
  --rate 0.25 \
  --output artifacts/student_10L_pruned
```

**DurÃ©e estimÃ©e**: 30-60 minutes

**Sorties**:
- `artifacts/student_10L_pruned/pytorch_model.bin`
- `artifacts/student_10L_pruned/heads_pruned_mask.json`

### 3. Fine-tuning post-pruning

RÃ©cupÃ©rer les performances aprÃ¨s pruning.

```bash
python finetune_postprune.py \
  --model artifacts/student_10L_pruned \
  --output artifacts/student_10L_final \
  --epochs 2
```

**DurÃ©e estimÃ©e**: 1-2 heures

### 4. InfÃ©rence

Extraire entitÃ©s NER sur nouveaux textes.

```bash
python inference.py \
  --model artifacts/student_10L_final \
  --input phrases.txt \
  --output entities.jsonl
```

**Format sortie** (entities.jsonl):
```json
{"text": "Jean habite Ã  Paris", "entities": [{"text": "Jean", "type": "PER", "start": 0, "end": 1}, {"text": "Paris", "type": "LOC", "start": 3, "end": 4}]}
```

## ğŸ–¥ï¸ Configuration RunPod

### 1. CrÃ©er instance RunPod

- **GPU**: RTX 4090 (24 GB) ou A100 (40/80 GB)
- **Template**: PyTorch 2.0+ CUDA 11.8
- **Volume**: 50 GB pour datasets et checkpoints

### 2. Installation dÃ©pendances

```bash
cd /workspace
git clone <your_repo>
cd training_ner
pip install -r requirements.txt
```

### 3. PrÃ©parer donnÃ©es

```bash
# Uploader vos donnÃ©es JSONL
# /workspace/data/train.jsonl
# /workspace/data/val.jsonl
# /workspace/data/test.jsonl
# /workspace/data/label2id.json
```

### 4. Lancer entraÃ®nement

```bash
# Adapter paths dans configs/kd_camembert.yaml
python train_kd.py --config configs/kd_camembert.yaml --output /workspace/artifacts/student_10L
```

## âš™ï¸ Configuration principale (kd_camembert.yaml)

Voir `configs/kd_camembert.yaml` pour configuration complÃ¨te.

**ParamÃ¨tres clÃ©s**:
- `teacher.model_name`: `Jean-Baptiste/camembert-ner`
- `student.num_layers`: `10` (rÃ©duit de 12)
- `distillation.temperature`: `2.5`
- `pruning.rate`: `0.25` (25% tÃªtes prunÃ©es)
- `training.batch_size`: `16`
- `training.learning_rate`: `2e-5`
- `training.max_epochs`: `10`

## ğŸ“Š Monitoring

Les mÃ©triques sont loggÃ©es dans:
- `artifacts/student_10L/training_log.jsonl` (par step)
- `artifacts/student_10L/training_summary.json` (rÃ©sumÃ©)

## ğŸ” Validation setup

Avant de lancer sur RunPod, valider localement:

```bash
# VÃ©rifier format donnÃ©es
python -c "from data_loader import verify_data_format; verify_data_format('data/train.jsonl')"

# VÃ©rifier config
python -c "from utils import load_config; print(load_config('configs/kd_camembert.yaml'))"
```

## ğŸ“ˆ Performances attendues

**ModÃ¨le teacher** (Jean-Baptiste/camembert-ner):
- ParamÃ¨tres: ~110M
- F1-score: ~90% (sur corpus franÃ§ais gÃ©nÃ©ral)

**ModÃ¨le student final** (10L + pruning 25%):
- ParamÃ¨tres: ~60-70M (compression ~1.5-1.8x)
- F1-score: ~85-88% (perte acceptable 2-5%)
- Latence: ~40-50% plus rapide

## â“ Troubleshooting

### Erreur: CUDA Out of Memory
- RÃ©duire `batch_size` dans config (16 â†’ 8)
- DÃ©sactiver `mixed_precision` si problÃ¨me

### Erreur: Teacher model not found
- VÃ©rifier connexion internet
- VÃ©rifier `teacher.model_name` dans config

### DonnÃ©es mal formatÃ©es
- VÃ©rifier format JSONL (une ligne = un JSON)
- VÃ©rifier longueur tokens = longueur ner_tags

## ğŸ“š RÃ©fÃ©rences

- **Patient KD**: https://arxiv.org/abs/1908.09355
- **CamemBERT**: https://arxiv.org/abs/1911.03894
- **Pruning Heads**: https://arxiv.org/abs/1905.10650

## ğŸ“ TODO pour RunPod

Les sections marquÃ©es `TODO: ImplÃ©menter sur RunPod` dans le code nÃ©cessitent implÃ©mentation complÃ¨te:
- [ ] Forward passes teacher/student avec extraction hidden states
- [ ] Calcul rÃ©el des 4 pertes (CE, KD, Hidden, CRF)
- [ ] Calcul importance tÃªtes d'attention (gradient Ã— activation)
- [ ] Masquage effectif des poids Q/K/V/O des tÃªtes prunÃ©es
- [ ] Chargement/sauvegarde checkpoints
- [ ] Extraction entitÃ©s NER avec alignement subwords

## ğŸ“ Support

Pour questions: [votre contact]
